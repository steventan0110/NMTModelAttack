{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testsuites:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will test COMET recommended metric against some sentence perturbations.\n",
    "\n",
    "- Lowercased MT hypotheis\n",
    "- Lack of punctuation\n",
    "- Random Word Drops\n",
    "- Typos\n",
    "- Word Swaps\n",
    "- No-Translation\n",
    "\n",
    "The goal is that our models rank the original MT higher than the corrrupted version.\n",
    "\n",
    "We will measure ties, and fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet.models import download_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = download_model(\"wmt-large-da-estimator-1719\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = [\n",
    "    \"Darbų sutartis buvo pasirašyta spalio 2 dieną.\",\n",
    "    \"I was sure I canceled, since I went through the entire process.\",\n",
    "    \"I don't see anything new changing.\",\n",
    "    \"中国人对俄罗斯文化有着浓厚的兴趣。\",\n",
    "    \"Sorry, I think it is good now.\",\n",
    "]\n",
    "\n",
    "hypothesis = [\n",
    "    \"The works contract was signed on 2 October.\",\n",
    "    \"Eu tenho certeza de que cancelei, dado que passei pelo processo todo.\",\n",
    "    \"Ich sehe nichts Neues, das sich ändert“.\",\n",
    "    \"The Chinese have a strong interest in Russian culture.\",\n",
    "    \"Ursäkta, jag tror att det är bra nu.\"\n",
    "]\n",
    "\n",
    "references = [\n",
    "    \"The contract for works was signed on 2 October.\",\n",
    "    \"Eu tinha a certeza que cancelei, pois passei pelo processo inteiro.\",\n",
    "    \"Ich sehe nichts Neues, das sich ändert.\",\n",
    "    \"Chinese people have a keen interest in Russian culture.\",\n",
    "    \"Tyvärr, jag tror att det är bra nu.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ties = 0\n",
    "fails = 0 \n",
    "tests = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = {\"src\": sources, \"mt\": hypothesis, \"ref\": references}\n",
    "samples = [dict(zip(samples, t)) for t in zip(*samples.values())]\n",
    "model.predict(samples)\n",
    "samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Lowercase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowercase_samples = {\"src\": sources, \"mt\": [h.lower() for h in hypothesis], \"ref\": references}\n",
    "lowercase_samples = [dict(zip(lowercase_samples, t)) for t in zip(*lowercase_samples.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(lowercase_samples)\n",
    "for i in range(len(lowercase_samples)):\n",
    "    if abs(lowercase_samples[i][\"predicted_score\"] - samples[i][\"predicted_score\"]) < threshold:\n",
    "        ties += 1\n",
    "    elif samples[i][\"predicted_score\"] - lowercase_samples[i][\"predicted_score\"] < threshold:\n",
    "        fail += 1\n",
    "    tests += 1\n",
    "    \n",
    "print (f\"{fails} fails and {ties} ties.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Punctuation Missing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lack_punct_samples = {\"src\": sources, \"mt\": [h[:-1] for h in hypothesis], \"ref\": references}\n",
    "lack_punct_samples = [dict(zip(lack_punct_samples, t)) for t in zip(*lack_punct_samples.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(lack_punct_samples)\n",
    "for i in range(len(lack_punct_samples)):\n",
    "    if abs(lack_punct_samples[i][\"predicted_score\"] - samples[i][\"predicted_score\"]) < threshold:\n",
    "        ties += 1\n",
    "    elif samples[i][\"predicted_score\"] - lack_punct_samples[i][\"predicted_score\"] < threshold:\n",
    "        fail += 1\n",
    "    tests += 1\n",
    "    \n",
    "print (f\"{fails} fails and {ties} ties.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Punctuation Change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_punct_samples = {\"src\": sources, \"mt\": [h[:-1]+'!' for h in hypothesis], \"ref\": references}\n",
    "change_punct_samples = [dict(zip(change_punct_samples, t)) for t in zip(*change_punct_samples.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(change_punct_samples)\n",
    "for i in range(len(change_punct_samples)):\n",
    "    if abs(change_punct_samples[i][\"predicted_score\"] - samples[i][\"predicted_score\"]) < threshold:\n",
    "        ties += 1\n",
    "    elif samples[i][\"predicted_score\"] - change_punct_samples[i][\"predicted_score\"] < threshold:\n",
    "        fail += 1\n",
    "    tests += 1\n",
    "    \n",
    "print (f\"{fails} fails and {ties} ties.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Word Drop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(3)\n",
    "wordrop_samples = []\n",
    "for s in samples:\n",
    "    mt = s['mt'].split(' ')\n",
    "    mt.pop(random.randrange(len(mt)))\n",
    "    wordrop_samples.append({'src': s['src'], 'mt': ' '.join(mt), 'ref': s['ref']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(wordrop_samples)\n",
    "for i in range(len(wordrop_samples)):\n",
    "    if abs(wordrop_samples[i][\"predicted_score\"] - samples[i][\"predicted_score\"]) < threshold:\n",
    "        ties += 1\n",
    "    elif samples[i][\"predicted_score\"] - wordrop_samples[i][\"predicted_score\"] < threshold:\n",
    "        fails += 1\n",
    "    tests += 1\n",
    "        \n",
    "print (f\"{fails} fails and {ties} ties.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, wordrop_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Typos:\n",
    "Test typos is not too important because usually MT does not generate typos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def add_typos(string, typos=1):\n",
    "    string = list(string)\n",
    "    swaps = np.random.choice(len(string) - 1, typos)\n",
    "    for swap in swaps:\n",
    "        tmp = string[swap]\n",
    "        string[swap] = string[swap + 1]\n",
    "        string[swap + 1] = tmp\n",
    "    return ''.join(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typos_samples = {\"src\": sources, \"mt\": [add_typos(h) for h in hypothesis], \"ref\": references}\n",
    "typos_samples = [dict(zip(typos_samples, t)) for t in zip(*typos_samples.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(typos_samples)\n",
    "for i in range(len(typos_samples)):\n",
    "    if abs(typos_samples[i][\"predicted_score\"] - samples[i][\"predicted_score\"]) < threshold:\n",
    "        ties += 1\n",
    "    elif samples[i][\"predicted_score\"] - typos_samples[i][\"predicted_score\"] < threshold:\n",
    "        fail += 1\n",
    "    tests += 1\n",
    "    \n",
    "print (f\"{fails} fails and {ties} ties.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Word Swap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def swap_words(string, swaps=1):\n",
    "    words = string.split()\n",
    "    swap_idx = np.random.choice(len(words) - 1, swaps)[0]\n",
    "    if swap_idx == 0:\n",
    "        swap_idx += 1\n",
    "    words[swap_idx-1], words[swap_idx] = words[swap_idx], words[swap_idx-1]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swap_samples = {\"src\": sources, \"mt\": [swap_words(h) for h in hypothesis], \"ref\": references}\n",
    "swap_samples = [dict(zip(swap_samples, t)) for t in zip(*swap_samples.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(swap_samples)\n",
    "for i in range(len(swap_samples)):\n",
    "    if abs(swap_samples[i][\"predicted_score\"] - samples[i][\"predicted_score\"]) < threshold:\n",
    "        ties += 1\n",
    "    elif samples[i][\"predicted_score\"] - swap_samples[i][\"predicted_score\"] < threshold:\n",
    "        fail += 1\n",
    "    tests += 1\n",
    "    \n",
    "print (f\"{fails} fails and {ties} ties.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test No-translate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_translate_samples = {\"src\": sources, \"mt\": sources, \"ref\": references}\n",
    "no_translate_samples = [dict(zip(no_translate_samples, t)) for t in zip(*no_translate_samples.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(no_translate_samples)\n",
    "for i in range(len(no_translate_samples)):\n",
    "    if abs(no_translate_samples[i][\"predicted_score\"] - samples[i][\"predicted_score\"]) < threshold:\n",
    "        ties += 1\n",
    "    elif samples[i][\"predicted_score\"] - no_translate_samples[i][\"predicted_score\"] < threshold:\n",
    "        fail += 1\n",
    "        \n",
    "print (f\"{fails} fails and {ties} ties.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Fail %: {}\".format(fails/tests))\n",
    "print (\"Ties %: {}\".format(ties/tests))\n",
    "print (\"Passed %: {}\".format((tests-(ties+fails))/tests))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
